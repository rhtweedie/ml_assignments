{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpy import array, zeros, exp, random, dot, shape, reshape, meshgrid, linspace\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "\n",
    "# for subplots within subplots:\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# for nice inset colorbars: (approach changed from lecture 1 'Visualization' notebook)\n",
    "from mpl_toolkits.axes_grid1.inset_locator import InsetPosition\n",
    "\n",
    "# for updating display \n",
    "# (very simple animation)\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "\n",
    "    # backpropagation and training routines\n",
    "\n",
    "    # this is basically a merger of the backpropagation\n",
    "    # code shown in lecture 2 and some of the \n",
    "    # visualization code used in the lecture 1 tutorials!\n",
    "\n",
    "    def __init__(self, nLayers, nBatch, layerSize, useRELU = False, wMax = 1, bMax = 1):\n",
    "        self.nlayers = nLayers\n",
    "        self.batchsize = nBatch\n",
    "        self.layersize = layerSize\n",
    "        self.useRELU = useRELU\n",
    "        \n",
    "        #Initialise the weights and biases with random numbers\n",
    "        self.w=[np.random.uniform(low = -1*wMax, high =+ 1*wMax, size = [self.layersize[j], self.layersize[j+1] ]) for j in range(self.nlayers)]\n",
    "        self.b=[np.random.uniform(low = -1*bMax, high =+ 1*bMax, size = [self.layersize[j+1]]) for j in range(self.nlayers)]\n",
    "        \n",
    "        #Define the arrays for various outputs needed for backpropagation\n",
    "        self.yArray = [np.zeros([self.batchsize, self.layersize[j]]) for j in range(self.nlayers + 1)]\n",
    "        self.dfArray = [np.zeros([self.batchsize, self.layersize[j+1]]) for j in range(self.nlayers)]\n",
    "        self.dwArray = [np.zeros([self.layersize[j], self.layersize[j+1]]) for j in range(self.nlayers)]\n",
    "        self.dbArray = [np.zeros(self.layersize[j+1]) for j in range(self.nlayers)]\n",
    "\n",
    "    def net_f_df(self, z, activation):\n",
    "        # return both value f(z) and derivative f'(z)\n",
    "        if activation=='sigmoid':\n",
    "            return([1/(1+np.exp(-z)), \n",
    "                    1/((1+np.exp(-z))*(1+np.exp(z))) ])\n",
    "        elif activation=='jump': # cheating a bit here: replacing f'(z)=delta(z) by something smooth\n",
    "            return([np.array(z>0,dtype='float'), \n",
    "                    10.0/((1+np.exp(-10*z))*(1+np.exp(10*z))) ] )\n",
    "        elif activation=='linear':\n",
    "            return([z,\n",
    "                    1.0])\n",
    "        elif activation=='reLU':\n",
    "            return([(z>0)*z,\n",
    "                    (z>0)*1.0\n",
    "                ])\n",
    "\n",
    "    def forward_step(self,y,w,b,activation):\n",
    "        \"\"\"\n",
    "        Go from one layer to the next, given a \n",
    "        weight matrix w (shape [n_neurons_in,n_neurons_out])\n",
    "        a bias vector b (length n_neurons_out)\n",
    "        and the values of input neurons y_in \n",
    "        (shape [batchsize,n_neurons_in])\n",
    "        \n",
    "        returns the values of the output neurons in the next layer \n",
    "        (shape [batchsize, n_neurons_out])\n",
    "        \"\"\"    \n",
    "        # calculate values in next layer, from input y\n",
    "        z=np.dot(y,w)+b # w=weights, b=bias vector for next layer\n",
    "        return(self.net_f_df(z,activation)) # apply nonlinearity and return result\n",
    "\n",
    "    def apply_net(self,x_in): # one forward pass through the network\n",
    "        global Weights, Biases, NumLayers, Activations\n",
    "        global y_layer, df_layer # for storing y-values and df/dz values\n",
    "        \n",
    "        y=np.copy(x_in) # start with input values\n",
    "        y_layer[0]=np.copy(y)\n",
    "        for j in range(NumLayers): # loop through all layers\n",
    "            # j=0 corresponds to the first layer above the input\n",
    "            y,df=self.forward_step(y,Weights[j],Biases[j],Activations[j]) # one step\n",
    "            df_layer[j]=np.copy(df) # store f'(z) [needed later in backprop]\n",
    "            y_layer[j+1]=np.copy(y) # store f(z) [also needed in backprop]        \n",
    "        return(y)\n",
    "\n",
    "    def apply_net_simple(self,x_in): # one forward pass through the network\n",
    "        # no storage for backprop (this is used for simple tests)\n",
    "        global Weights, Biases, NumLayers, Activations\n",
    "        \n",
    "        y=x_in # start with input values\n",
    "        for j in range(NumLayers): # loop through all layers\n",
    "            # j=0 corresponds to the first layer above the input\n",
    "            y,df=self.forward_step(y,Weights[j],Biases[j],Activations[j]) # one step\n",
    "        return(y)\n",
    "\n",
    "    def backward_step(self,delta,w,df): \n",
    "        # delta at layer N, of batchsize x layersize(N))\n",
    "        # w between N-1 and N [layersize(N-1) x layersize(N) matrix]\n",
    "        # df = df/dz at layer N-1, of batchsize x layersize(N-1)\n",
    "        return( np.dot(delta,np.transpose(w))*df )\n",
    "\n",
    "    def backprop(self,y_target): # one backward pass through the network\n",
    "        # the result will be the 'dw_layer' matrices that contain\n",
    "        # the derivatives of the cost function with respect to\n",
    "        # the corresponding weight\n",
    "        global y_layer, df_layer, Weights, Biases, NumLayers\n",
    "        global dw_layer, db_layer # dCost/dw and dCost/db (w,b=weights,biases)\n",
    "\n",
    "        batchsize=np.shape(y_target)[0]\n",
    "        delta=(y_layer[-1]-y_target)*df_layer[-1]\n",
    "        dw_layer[-1]=np.dot(np.transpose(y_layer[-2]),delta)/batchsize\n",
    "        db_layer[-1]=delta.sum(0)/batchsize\n",
    "        for j in range(NumLayers-1):\n",
    "            delta=self.backward_step(delta,Weights[-1-j],df_layer[-2-j])\n",
    "            dw_layer[-2-j]=np.dot(np.transpose(y_layer[-3-j]),delta)/batchsize # batchsize was missing in old code?\n",
    "            db_layer[-2-j]=delta.sum(0)/batchsize\n",
    "            \n",
    "    def gradient_step(self,eta): # update weights & biases (after backprop!)\n",
    "        global dw_layer, db_layer, Weights, Biases\n",
    "        \n",
    "        for j in range(NumLayers):\n",
    "            Weights[j]-=eta*dw_layer[j]\n",
    "            Biases[j]-=eta*db_layer[j]\n",
    "            \n",
    "    def train_net(self,x_in,y_target,eta): # one full training batch\n",
    "        # x_in is an array of size batchsize x (input-layer-size)\n",
    "        # y_target is an array of size batchsize x (output-layer-size)\n",
    "        # eta is the stepsize for the gradient descent\n",
    "        global y_out_result\n",
    "        \n",
    "        y_out_result=self.apply_net(x_in)\n",
    "        self.backprop(y_target)\n",
    "        self.gradient_step(eta)\n",
    "        cost=0.5*((y_target-y_out_result)**2).sum()/np.shape(x_in)[0]\n",
    "        return(cost)\n",
    "\n",
    "    def init_layer_variables(self,weights,biases,activations):\n",
    "        global Weights, Biases, NumLayers, Activations\n",
    "        global LayerSizes, y_layer, df_layer, dw_layer, db_layer\n",
    "\n",
    "        Weights=weights\n",
    "        Biases=biases\n",
    "        Activations=activations\n",
    "        NumLayers=len(Weights)\n",
    "\n",
    "        LayerSizes=[2]\n",
    "        for j in range(NumLayers):\n",
    "            LayerSizes.append(len(Biases[j]))\n",
    "\n",
    "        y_layer=[[] for j in range(NumLayers+1)]\n",
    "        df_layer=[[] for j in range(NumLayers)]\n",
    "        dw_layer=[np.zeros([LayerSizes[j],LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "        db_layer=[np.zeros(LayerSizes[j+1]) for j in range(NumLayers)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1042528550f16a04ea686e80501a57f389ea03468b81825b518e71bb9210b4ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
