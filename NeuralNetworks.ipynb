{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2: Visualizing Neural Networks - Circle Functions\n",
    "\n",
    "#### Heather Tweedie, 26/01/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpy import array, zeros, exp, random, dot, shape, reshape, meshgrid, linspace\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "\n",
    "# for subplots within subplots:\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# for nice inset colorbars: (approach changed from lecture 1 'Visualization' notebook)\n",
    "from mpl_toolkits.axes_grid1.inset_locator import InsetPosition\n",
    "\n",
    "# for updating display \n",
    "# (very simple animation)\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for backpropogation and training. The functions used are taken from the exercise description notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation and training routines\n",
    "\n",
    "# this is basically a merger of the backpropagation\n",
    "# code shown in lecture 2 and some of the \n",
    "# visualization code used in the lecture 1 tutorials!\n",
    "\n",
    "def net_f_df(z,activation):\n",
    "    # return both value f(z) and derivative f'(z)\n",
    "    if activation=='sigmoid':\n",
    "        return([1/(1+np.exp(-z)), \n",
    "                1/((1+np.exp(-z))*(1+np.exp(z))) ])\n",
    "    elif activation=='jump': # cheating a bit here: replacing f'(z)=delta(z) by something smooth\n",
    "        return([np.array(z>0,dtype='float'), \n",
    "                10.0/((1+np.exp(-10*z))*(1+np.exp(10*z))) ] )\n",
    "    elif activation=='linear':\n",
    "        return([z,\n",
    "                1.0])\n",
    "    elif activation=='reLU':\n",
    "        return([(z>0)*z,\n",
    "                (z>0)*1.0\n",
    "               ])\n",
    "\n",
    "def forward_step(y,w,b,activation):\n",
    "    \"\"\"\n",
    "    Go from one layer to the next, given a \n",
    "    weight matrix w (shape [n_neurons_in,n_neurons_out])\n",
    "    a bias vector b (length n_neurons_out)\n",
    "    and the values of input neurons y_in \n",
    "    (shape [batchsize,n_neurons_in])\n",
    "    \n",
    "    returns the values of the output neurons in the next layer \n",
    "    (shape [batchsize, n_neurons_out])\n",
    "    \"\"\"    \n",
    "    # calculate values in next layer, from input y\n",
    "    z=np.dot(y,w)+b # w=weights, b=bias vector for next layer\n",
    "    return(net_f_df(z,activation)) # apply nonlinearity and return result\n",
    "\n",
    "def apply_net(x_in): # one forward pass through the network\n",
    "    global Weights, Biases, NumLayers, Activations\n",
    "    global y_layer, df_layer # for storing y-values and df/dz values\n",
    "    \n",
    "    y=np.copy(x_in) # start with input values\n",
    "    y_layer[0]=np.copy(y)\n",
    "    for j in range(NumLayers): # loop through all layers\n",
    "        # j=0 corresponds to the first layer above the input\n",
    "        y,df=forward_step(y,Weights[j],Biases[j],Activations[j]) # one step\n",
    "        df_layer[j]=np.copy(df) # store f'(z) [needed later in backprop]\n",
    "        y_layer[j+1]=np.copy(y) # store f(z) [also needed in backprop]        \n",
    "    return(y)\n",
    "\n",
    "def apply_net_simple(x_in): # one forward pass through the network\n",
    "    # no storage for backprop (this is used for simple tests)\n",
    "    global Weights, Biases, NumLayers, Activations\n",
    "    \n",
    "    y=x_in # start with input values\n",
    "    for j in range(NumLayers): # loop through all layers\n",
    "        # j=0 corresponds to the first layer above the input\n",
    "        y,df=forward_step(y,Weights[j],Biases[j],Activations[j]) # one step\n",
    "    return(y)\n",
    "\n",
    "def backward_step(delta,w,df): \n",
    "    # delta at layer N, of batchsize x layersize(N))\n",
    "    # w between N-1 and N [layersize(N-1) x layersize(N) matrix]\n",
    "    # df = df/dz at layer N-1, of batchsize x layersize(N-1)\n",
    "    return( np.dot(delta,np.transpose(w))*df )\n",
    "\n",
    "def backprop(y_target): # one backward pass through the network\n",
    "    # the result will be the 'dw_layer' matrices that contain\n",
    "    # the derivatives of the cost function with respect to\n",
    "    # the corresponding weight\n",
    "    global y_layer, df_layer, Weights, Biases, NumLayers\n",
    "    global dw_layer, db_layer # dCost/dw and dCost/db (w,b=weights,biases)\n",
    "\n",
    "    batchsize=np.shape(y_target)[0]\n",
    "    delta=(y_layer[-1]-y_target)*df_layer[-1]\n",
    "    dw_layer[-1]=np.dot(np.transpose(y_layer[-2]),delta)/batchsize\n",
    "    db_layer[-1]=delta.sum(0)/batchsize\n",
    "    for j in range(NumLayers-1):\n",
    "        delta=backward_step(delta,Weights[-1-j],df_layer[-2-j])\n",
    "        dw_layer[-2-j]=np.dot(np.transpose(y_layer[-3-j]),delta)/batchsize # batchsize was missing in old code?\n",
    "        db_layer[-2-j]=delta.sum(0)/batchsize\n",
    "        \n",
    "def gradient_step(eta): # update weights & biases (after backprop!)\n",
    "    global dw_layer, db_layer, Weights, Biases\n",
    "    \n",
    "    for j in range(NumLayers):\n",
    "        Weights[j]-=eta*dw_layer[j]\n",
    "        Biases[j]-=eta*db_layer[j]\n",
    "        \n",
    "def train_net(x_in,y_target,eta): # one full training batch\n",
    "    # x_in is an array of size batchsize x (input-layer-size)\n",
    "    # y_target is an array of size batchsize x (output-layer-size)\n",
    "    # eta is the stepsize for the gradient descent\n",
    "    global y_out_result\n",
    "    \n",
    "    y_out_result=apply_net(x_in)\n",
    "    backprop(y_target)\n",
    "    gradient_step(eta)\n",
    "    cost=0.5*((y_target-y_out_result)**2).sum()/np.shape(x_in)[0]\n",
    "    return(cost)\n",
    "\n",
    "def init_layer_variables(weights,biases,activations):\n",
    "    global Weights, Biases, NumLayers, Activations\n",
    "    global LayerSizes, y_layer, df_layer, dw_layer, db_layer\n",
    "\n",
    "    Weights=weights\n",
    "    Biases=biases\n",
    "    Activations=activations\n",
    "    NumLayers=len(Weights)\n",
    "\n",
    "    LayerSizes=[2]\n",
    "    for j in range(NumLayers):\n",
    "        LayerSizes.append(len(Biases[j]))\n",
    "\n",
    "    y_layer=[[] for j in range(NumLayers+1)]\n",
    "    df_layer=[[] for j in range(NumLayers)]\n",
    "    dw_layer=[np.zeros([LayerSizes[j],LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "    db_layer=[np.zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for visualising the network training. The functions used are taken from the exercise description notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization routines:\n",
    "\n",
    "# some internal routines for plotting the network:\n",
    "def plot_connection_line(ax,X,Y,W,vmax=1.0,linewidth=3):\n",
    "    t=np.linspace(0,1,20)\n",
    "    if W>0:   #Pick colour of line based on if weight is positive or negative\n",
    "        col=[0,0.4,0.8]  \n",
    "    else:\n",
    "        col=[1,0.3,0]\n",
    "    ax.plot(X[0]+(3*t**2-2*t**3)*(X[1]-X[0]),Y[0]+t*(Y[1]-Y[0]),\n",
    "           alpha=abs(W)/vmax,color=col,\n",
    "           linewidth=linewidth)\n",
    "    \n",
    "def plot_neuron_alpha(ax,X,Y,B,size=100.0,vmax=1.0):\n",
    "    if B>0: #Pick colour of neuron dot based on if bias is positive or negative\n",
    "        col=[0,0.4,0.8]\n",
    "    else:\n",
    "        col=[1,0.3,0]\n",
    "    ax.scatter([X],[Y],marker='o',c=np.atleast_2d([col]),alpha=abs(B)/vmax,s=size,zorder=10)\n",
    "\n",
    "def plot_neuron(ax,X,Y,B,size=100.0,vmax=1.0):\n",
    "    if B>0:\n",
    "        col=[0,0.4,0.8]\n",
    "    else:\n",
    "        col=[1,0.3,0]\n",
    "    ax.scatter([X],[Y],marker='o',c=np.atleast_2d([col]),s=size,zorder=10)\n",
    "    \n",
    "def visualize_network(weights,biases,activations,\n",
    "                      M=100,x0range=[-1,1],x1range=[-1,1],\n",
    "                     size=400.0, linewidth=5.0,\n",
    "                     weights_are_swapped=False,\n",
    "                    layers_already_initialized=False,\n",
    "                      plot_cost_function=None,\n",
    "                      current_cost=None, cost_max=None, plot_target=None\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    Visualize a neural network with 2 input \n",
    "    neurons and 1 output neuron (plot output vs input in a 2D plot)\n",
    "    \n",
    "    weights is a list of the weight matrices for the\n",
    "    layers, where weights[j] is the matrix for the connections\n",
    "    from layer j to layer j+1 (where j==0 is the input)\n",
    "    \n",
    "    weights[j][m,k] is the weight for input neuron k going to output neuron m\n",
    "    (note: internally, m and k are swapped, see the explanation of\n",
    "    batch processing in lecture 2)\n",
    "    \n",
    "    biases[j] is the vector of bias values for obtaining the neurons in layer j+1\n",
    "    biases[j][k] is the bias for neuron k in layer j+1\n",
    "    \n",
    "    activations is a list of the activation functions for\n",
    "    the different layers: choose 'linear','sigmoid',\n",
    "    'jump' (i.e. step-function), and 'reLU'\n",
    "    \n",
    "    M is the resolution (MxM grid)\n",
    "    \n",
    "    x0range is the range of x0 neuron values (horizontal axis)\n",
    "    x1range is the range of x1 neuron values (vertical axis)\n",
    "    \"\"\"\n",
    "    if not weights_are_swapped:\n",
    "        swapped_weights=[]\n",
    "        for j in range(len(weights)):\n",
    "            swapped_weights.append(np.transpose(weights[j]))\n",
    "    else:\n",
    "        swapped_weights=weights\n",
    "\n",
    "    x0,x1=np.meshgrid(np.linspace(x0range[0],x0range[1],M),np.linspace(x1range[0],x1range[1],M))\n",
    "    x_in=np.zeros([M*M,2])\n",
    "    x_in[:,0]=x0.flatten()\n",
    "    x_in[:,1]=x1.flatten()\n",
    "    \n",
    "    # if we call visualization directly, we still\n",
    "    # need to initialize the 'Weights' and other\n",
    "    # global variables; otherwise (during training)\n",
    "    # all of this has already been taken care of:\n",
    "    if not layers_already_initialized:\n",
    "        init_layer_variables(swapped_weights,biases,activations)\n",
    "    y_out=apply_net_simple(x_in)\n",
    "\n",
    "    if plot_cost_function is None:\n",
    "        fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(8,4))\n",
    "    else:\n",
    "        fig=plt.figure(figsize=(8,4))\n",
    "        gs_top = gridspec.GridSpec(nrows=1, ncols=2)\n",
    "        gs_left = gridspec.GridSpecFromSubplotSpec(nrows=2, ncols=1, subplot_spec=gs_top[0], height_ratios=[1.0,0.3])\n",
    "        ax=[ fig.add_subplot(gs_left[0]),\n",
    "            fig.add_subplot(gs_top[1]),\n",
    "           fig.add_subplot(gs_left[1]) ]\n",
    "        # ax[0] is network\n",
    "        # ax[1] is image produced by network\n",
    "        # ax[2] is cost function subplot\n",
    "        \n",
    "    # plot the network itself:\n",
    "    \n",
    "    # positions of neurons on plot:\n",
    "    posX=[[-0.5,+0.5]]; posY=[[0,0]]\n",
    "    vmax=0.0 # for finding the maximum weight\n",
    "    vmaxB=0.0 # for maximum bias\n",
    "    for j in range(len(biases)):\n",
    "        n_neurons=len(biases[j])\n",
    "        posX.append(np.array(range(n_neurons))-0.5*(n_neurons-1))\n",
    "        posY.append(np.full(n_neurons,j+1))\n",
    "        vmax=np.maximum(vmax,np.max(np.abs(weights[j])))\n",
    "        vmaxB=np.maximum(vmaxB,np.max(np.abs(biases[j])))\n",
    "\n",
    "    # plot connections\n",
    "    for j in range(len(biases)):\n",
    "        for k in range(len(posX[j])):\n",
    "            for m in range(len(posX[j+1])):\n",
    "                plot_connection_line(ax[0],[posX[j][k],posX[j+1][m]],\n",
    "                                     [posY[j][k],posY[j+1][m]],\n",
    "                                     swapped_weights[j][k,m],vmax=vmax,\n",
    "                                    linewidth=linewidth)\n",
    "    \n",
    "    # plot neurons\n",
    "    for k in range(len(posX[0])): # input neurons (have no bias!)\n",
    "        plot_neuron(ax[0],posX[0][k],posY[0][k],\n",
    "                   vmaxB,vmax=vmaxB,size=size)\n",
    "    for j in range(len(biases)): # all other neurons\n",
    "        for k in range(len(posX[j+1])):\n",
    "            plot_neuron(ax[0],posX[j+1][k],posY[j+1][k],\n",
    "                       biases[j][k],vmax=vmaxB,size=size)\n",
    "    \n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    # now: the output of the network\n",
    "    img=ax[1].imshow(np.reshape(y_out,[M,M]),origin='lower',\n",
    "                    extent=[x0range[0],x0range[1],x1range[0],x1range[1]])\n",
    "    ax[1].set_xlabel(r'$x_0$')\n",
    "    ax[1].set_ylabel(r'$x_1$')\n",
    "    \n",
    "#     axins1 = inset_axes(ax[1],\n",
    "#                     width=\"40%\",  # width = 50% of parent_bbox width\n",
    "#                     height=\"5%\",  # height : 5%\n",
    "#                     loc='upper right',\n",
    "#                        bbox_to_anchor=[0.3,0.4])\n",
    "\n",
    "#    axins1 = ax[1].inset_axes([0.5,0.8,0.45,0.1])\n",
    "    axins1 = plt.axes([0, 0, 1, 1])\n",
    "    ip = InsetPosition(ax[1], [0.25, 0.1, 0.5, 0.05])\n",
    "    axins1.set_axes_locator(ip)\n",
    "\n",
    "    imgmin=np.min(y_out)\n",
    "    imgmax=np.max(y_out)\n",
    "    color_bar=fig.colorbar(img, cax=axins1, orientation=\"horizontal\",ticks=np.linspace(imgmin,imgmax,3))\n",
    "    cbxtick_obj = plt.getp(color_bar.ax.axes, 'xticklabels')\n",
    "    plt.setp(cbxtick_obj, color=\"white\")\n",
    "    axins1.xaxis.set_ticks_position(\"bottom\")\n",
    "\n",
    "    if plot_target is not None:\n",
    "        axins2 = plt.axes([0.01, 0.01, 0.99, 0.99])\n",
    "        ip = InsetPosition(ax[1], [0.75, 0.75, 0.2, 0.2])\n",
    "        axins2.set_axes_locator(ip)\n",
    "        axins2.imshow(plot_target,origin='lower')\n",
    "        axins2.get_xaxis().set_ticks([])\n",
    "        axins2.get_yaxis().set_ticks([])\n",
    "        \n",
    "    if plot_cost_function is not None:\n",
    "        ax[2].plot(plot_cost_function)\n",
    "        ax[2].set_ylim([0.0,cost_max])\n",
    "        ax[2].set_yticks([0.0,cost_max])\n",
    "        ax[2].set_yticklabels([\"0\",'{:1.2e}'.format(cost_max)])\n",
    "        if current_cost is not None:\n",
    "            ax[2].text(0.9, 0.9, 'cost={:1.2e}'.format(current_cost), horizontalalignment='right',\n",
    "                       verticalalignment='top', transform=ax[2].transAxes)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def visualize_network_training(weights,biases,activations,\n",
    "                               target_function,\n",
    "                               num_neurons=None,\n",
    "                               weight_scale=1.0,\n",
    "                               bias_scale=1.0,\n",
    "                               xspread=1.0,\n",
    "                      M=100,x0range=[-1,1],x1range=[-1,1],\n",
    "                     size=400.0, linewidth=5.0,\n",
    "                    steps=100, batchsize=10, eta=0.1,\n",
    "                              random_init=False,\n",
    "                              visualize_nsteps=1,\n",
    "                              plot_target=True):\n",
    "    \"\"\"\n",
    "    Visualize the training of a neural network.\n",
    "    \n",
    "    weights, biases, and activations define the neural network \n",
    "    (the starting point of the optimization; for the detailed description,\n",
    "    see the help for visualize_network)\n",
    "    \n",
    "    If you want to have layers randomly initialized, just provide\n",
    "    the number of neurons for each layer as 'num_neurons'. This should include\n",
    "    all layers, including input (2 neurons) and output (1), so num_neurons=[2,3,5,4,1] is\n",
    "    a valid example. In this case, weight_scale and bias_scale define the\n",
    "    spread of the random Gaussian variables used to initialize all weights and biases.\n",
    "    \n",
    "    target_function is the name of the function that we\n",
    "    want to approximate; it must be possible to \n",
    "    evaluate this function on a batch of samples, by\n",
    "    calling target_function(y) on an array y of \n",
    "    shape [batchsize,2], where\n",
    "    the second index refers to the two coordinates\n",
    "    (input neuron values) x0 and x1. The return\n",
    "    value must be an array with one index, corresponding\n",
    "    to the batchsize. A valid example is:\n",
    "    \n",
    "    def my_target(y):\n",
    "        return( np.sin(y[:,0]) + np.cos(y[:,1]) )\n",
    "    \n",
    "    steps is the number of training steps\n",
    "    \n",
    "    batchsize is the number of samples per training step\n",
    "    \n",
    "    eta is the learning rate (stepsize in the gradient descent)\n",
    "    \n",
    "    xspread denotes the spread of the Gaussian\n",
    "    used to sample points in (x0,x1)-space\n",
    "    \n",
    "    visualize_n_steps>1 means skip some steps before\n",
    "    visualizing again (can speed up things)\n",
    "    \n",
    "    plot_target=True means do plot the target function in a corner\n",
    "    \n",
    "    For all the other parameters, see the help for\n",
    "        visualize_network\n",
    "    \n",
    "    weights and biases as given here will be used\n",
    "    as starting points, unless you specify\n",
    "    random_init=True, in which case they will be\n",
    "    used to determine the spread of Gaussian random\n",
    "    variables used for initialization!\n",
    "    \"\"\"\n",
    "    \n",
    "    if num_neurons is not None: # build weight matrices as randomly initialized\n",
    "        weights=[weight_scale*np.random.randn(num_neurons[j+1],num_neurons[j]) for j in range(len(num_neurons)-1)]\n",
    "        biases=[bias_scale*np.random.randn(num_neurons[j+1]) for j in range(len(num_neurons)-1)]\n",
    "    \n",
    "    swapped_weights=[]\n",
    "    for j in range(len(weights)):\n",
    "        swapped_weights.append(np.transpose(weights[j]))\n",
    "    init_layer_variables(swapped_weights,biases,activations)\n",
    "    \n",
    "    if plot_target:\n",
    "        x0,x1=np.meshgrid(np.linspace(x0range[0],x0range[1],M),np.linspace(x1range[0],x1range[1],M))\n",
    "        y=np.zeros([M*M,2])\n",
    "        y[:,0]=x0.flatten()\n",
    "        y[:,1]=x1.flatten()\n",
    "        plot_target_values=np.reshape(target_function(y),[M,M])\n",
    "    else:\n",
    "        plot_target_values=None\n",
    "    \n",
    "    y_target=np.zeros([batchsize,1])\n",
    "    costs=np.zeros(steps)\n",
    "    \n",
    "    for j in range(steps):\n",
    "        # produce samples (random points in x0,x1-space):\n",
    "        x_in=xspread*np.random.randn(batchsize,2)\n",
    "        # apply target function to those points:\n",
    "        y_target[:,0]=target_function(x_in)\n",
    "        # do one training step on this batch of samples:\n",
    "        costs[j]=train_net(x_in,y_target,eta)\n",
    "        \n",
    "        # now visualize the updated network:\n",
    "        if j%visualize_nsteps==0:\n",
    "            clear_output(wait=True) # for animation\n",
    "            if j>10:\n",
    "                cost_max=np.average(costs[0:j])*1.5\n",
    "            else:\n",
    "                cost_max=costs[0]\n",
    "            visualize_network(Weights,Biases,activations,\n",
    "                          M,x0range=x0range,x1range=x1range,\n",
    "                         size=size, linewidth=linewidth,\n",
    "                             weights_are_swapped=True,\n",
    "                             layers_already_initialized=True,\n",
    "                             plot_cost_function=costs,\n",
    "                             current_cost=costs[j],\n",
    "                             cost_max=cost_max,\n",
    "                             plot_target=plot_target_values)\n",
    "            sleep(0.1) # wait a bit before next step (probably not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the parameters below, I achieved a cost of 6.33 e-3. I initially used 2000 steps, however no substantial reduction in the cost occured after 1500 steps, so I reduced it to that. The only difference this made was increasing the size of the 'halo' of green around the yellow circle. It appeared from trying various combinations of layer sizes that the first layer influenced the smoothness of the circumference of the circle - using a layer size of five to begin with, the circumference seemed to be made of 5 or 6 straight lines, making it appear more like a pentagon or hexagon. Increasing the size of the first layer increased the smoothness of this. \n",
    "\n",
    "I was also initially using a deeper network with four layers, however I found that reducing this to three layers reduced the cost from 8.5 e-3 to 6.33 e-3, as well as reducing the run time. I tried different combinations of activation functions, but found that using all reLU gave the best results. I thought that having a sigmoid function in the final layer would smooth the circle edges more, however this only increased the size of the 'halo' around the circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_target(y):\n",
    "    R=2.0\n",
    "    return(1.0 * (y[:,0] ** 2 + y[:,1] ** 2 < R**2 ) )\n",
    "    \n",
    "visualize_network_training(weights=[],biases=[],\n",
    "    num_neurons=[2,20,5,1], # this generates randomly initialized layers of the given neuron numbers!\n",
    "    bias_scale=0.0, weight_scale=0.1, # the scale of the random numbers\n",
    "    target_function=my_target, # the target function to approximate\n",
    "    activations=[ 'reLU',\n",
    "                 'reLU',\n",
    "                 'reLU'            \n",
    "                ],\n",
    "    x0range=[-3,3],x1range=[-3,3],\n",
    "    xspread=3,\n",
    "    steps=1500, eta=.1, batchsize=200,\n",
    "                          visualize_nsteps=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circle with hole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, I used a combination of reLU and jump activation functions, which achieved the general shape but was blocky with sharp-edged shapes of block colour. Adding the sigmoid function at the final layer of the neural network smoothed out the edges while retaining the shape. The jump actication function was necessary for the network to form the hole. \n",
    "\n",
    "The number of steps is large, but with these parameters it was required to achieve a low cost. There was no substantial reduction in cost above 3000 steps. I tried using a variety of weights alongside the network layer combinations, and found that 0.2 gave the closestr result. Below this, and the network did not successfully form the hole in the middle of the circle, and remained mostly filled in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_target(y):\n",
    "    r=1.0; R=2.0\n",
    "    return( 1.0*( y[:,0]**2+y[:,1]**2<R**2 ) - 1.0*( (y[:,0])**2+(y[:,1])**2<r**2))\n",
    "    \n",
    "visualize_network_training(weights=[],biases=[],\n",
    "    num_neurons=[2,25,15,25,1], # this generates randomly initialized layers of the given neuron numbers!\n",
    "    bias_scale=0.0, weight_scale=0.2, # the scale of the random numbers\n",
    "    target_function=my_target, # the target function to approximate\n",
    "    activations=['reLU',\n",
    "                 'reLU',\n",
    "                 'jump',\n",
    "                 'sigmoid'          \n",
    "                ],\n",
    "    x0range=[-3,3],x1range=[-3,3],\n",
    "    xspread=3,\n",
    "    steps=3000, eta=.1, batchsize=200,\n",
    "                          visualize_nsteps=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'The scream' was less successful. Despite trying a large number of parameters and making relatively educated guesses about what may work best, I was not able to achieve much more than a blocky circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_target(x): #A circle of radius 2, two eyes of radius 0.5 and a mouth of radius 0.8\n",
    "    a=0.8; r=0.5; R=2.0; r2=0.8\n",
    "    return( 1.0*( x[:,0]**2+x[:,1]**2<R**2 ) - 1.0*( (x[:,0]-a)**2+(x[:,1]-a)**2<r**2) - 1.0*( (x[:,0]+a)**2+(x[:,1]-a)**2<r**2 ) - 1.0*( (x[:,0])**2+(x[:,1]+a)**2<r2**2 ) )\n",
    "\n",
    "visualize_network_training(weights=[],biases=[],\n",
    "    num_neurons=[2, 40, 10, 20, 1], # this generates randomly initialized layers of the given neuron numbers!\n",
    "    bias_scale=0.0, weight_scale=0.15, # the scale of the random numbers\n",
    "    target_function=my_target, # the target function to approximate\n",
    "    activations=[ 'reLU',\n",
    "                 'linear',\n",
    "                 'reLU',\n",
    "                 'sigmoid'             \n",
    "                ],\n",
    "    x0range=[-3,3],x1range=[-3,3],\n",
    "    xspread=3,\n",
    "    steps=2000, eta=.1, batchsize=600,\n",
    "                          visualize_nsteps=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fdcffc973ed0b65d6c6680211394d0c134db5f0e6a4d5de6db995f953f5325c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
