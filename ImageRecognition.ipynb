{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3 - Image Recognition\n",
    "\n",
    "### Heather Tweedie, 1/2/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.style #Some style nonsense\n",
    "import matplotlib as mpl #Some more style nonsense\n",
    "\n",
    "#Set default figure size\n",
    "#mpl.rcParams['figure.figsize'] = [12.0, 8.0] #Inches... of course it is inches\n",
    "mpl.rcParams[\"legend.frameon\"] = False\n",
    "mpl.rcParams['figure.dpi']=200 # dots per inch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the MNIST dataset from Keras, set the image scales correctly, and define the size of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# re-scale inputs\n",
    "train_images=train_images/255.0\n",
    "test_images=test_images/255.0\n",
    "\n",
    "# check shape of datasets\n",
    "print(\"Shape of training images:\",train_images.shape)\n",
    "print(\"Length of training set labels:\",len(train_labels))\n",
    "print(\"First label:\",train_labels[0])\n",
    "print(\"Shape of testing images:\",test_images.shape)\n",
    "print(\"Length of testing set labels:\",len(test_labels))\n",
    "\n",
    "image_x = len(train_images[0,:,0])\n",
    "image_y = len(train_images[0,0,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a number of functions for use in training and testing the model. `addNoise` adds a random noise contribution to each pixel of an image; `trainWithNoise` trains a model on noisy images; and `accuracyWithNoise` evaluates the accuracy of a model, testing it on noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNoise(image, y_noise):\n",
    "    \"\"\"\n",
    "    Adds a random noise contribution drawn from a uniform distribution between 0 and a user-defined maximum to an image.\n",
    "    \n",
    "    Args:\n",
    "        image: the image to which the noise will be added\n",
    "        y_noise: the maximum value for the uniform distribution from which the random noise contribution will be drawn\n",
    "\n",
    "    Returns:\n",
    "        newImage: the new image with noise added\n",
    "    \"\"\"\n",
    "    # get image dimensions\n",
    "    image_x = 28\n",
    "    image_y = 28\n",
    "\n",
    "    newImage = np.empty([image_x, image_y])\n",
    "    for i in range(image_x):\n",
    "        for j in range(image_y):\n",
    "            newImage[i,j] = image[i,j] + np.random.uniform(0, y_noise)\n",
    "\n",
    "    return newImage\n",
    "\n",
    "    \n",
    "\n",
    "def trainWithNoise(model, image_x, image_y, y_noise, batchSize, nepochs):\n",
    "    \"\"\"\n",
    "    Trains a model on MNIST image data with noise added, and evaluates it.\n",
    "    \n",
    "    Args:\n",
    "        model: the model to be trained\n",
    "        image_x: the x length of the image\n",
    "        image_y: the y length of the image\n",
    "        y_noise: the maximum value for the uniform distribution from which the random noise contribution will be drawn\n",
    "        batchSize: the number of samples to be drawn during each step of the training\n",
    "        nepochs: the number of epochs of training to be carried out\n",
    "    \n",
    "    Returns:\n",
    "        test_loss: the cost function after training\n",
    "        test_acc: the accuracy of the model based on testing on a test set of images\n",
    "    \"\"\"\n",
    "\n",
    "    imagesWithNoise = np.empty([60000, image_x, image_y])\n",
    "    for i in range(len(imagesWithNoise[:,0,0])):\n",
    "        imagesWithNoise[i,:,:] = addNoise(train_images[i,:,:], y_noise)\n",
    "\n",
    "    history = model.fit(imagesWithNoise, train_labels, batch_size = batchSize, epochs = nepochs)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "\n",
    "def accuracyWithNoise(model, image_x, image_y, y_noise):\n",
    "    \"\"\"\n",
    "    Evaluates a model trained on MNIST data on a test dataset with noise added.\n",
    "    \n",
    "    Args:\n",
    "        model: the model to be evaluated\n",
    "        image_x: the x length of the image\n",
    "        image_y: the y length of the image\n",
    "        y_noise: the maximum value for the uniform distribution from which the random noise contribution will be drawn\n",
    "    \n",
    "    Returns:\n",
    "        test_loss: the cost of the model\n",
    "        test_acc: the accuracy of the model\n",
    "    \"\"\"\n",
    "\n",
    "    imagesWithNoise = np.empty([10000, image_x, image_y])\n",
    "    for i in range(len(imagesWithNoise[:,0,0])):\n",
    "        imagesWithNoise[i,:,:] = addNoise(test_images[i,:,:], y_noise)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(imagesWithNoise, test_labels, verbose=2)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define and compile the network on which we will train the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(128,activation='relu'),\n",
    "    keras.layers.Dense(15)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.SGD(learning_rate=1.0),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on the MNIST dataset. Initially, using the parameters provided in the course material (layers: [128, 10]; batch size = 100, epochs = 30), the model achieved an accuracy of around 0.94, which is lower than the acceptable level of 0.95. I tried increasing the batch size to 120 and then 140, but this only achieved an inconsistent 0.945 - 0.95. By increasing the size of the final layer in the network from 10 to 15, the accuracy of the model grew substantially, achieving an accuracy of 0.9998 - 1 on the trianing dataset. In order to make the model training faster, I reduced the number of epochs from 30 to 10, as at this point the model was already achieving high accuracies, and reduced the batch size back down to 100. This also reduced the likelihood of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels, batch_size=100, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate accuracy of model for different noise levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model accuracy against noisy data\n",
    "noises = np.linspace(0, 2, 20)\n",
    "accuracies = np.empty([len(noises)])\n",
    "\n",
    "for i in range(len(noises)):\n",
    "    print(f\"y_noise = {noises[i]}:\")\n",
    "    test_loss, test_acc = accuracyWithNoise(model, image_x, image_y, noises[i])\n",
    "    accuracies[i] = test_acc\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(noises, accuracies)\n",
    "ax.set_xlabel('Maximum noise level')\n",
    "ax.set_ylabel('Model accuracy')\n",
    "ax.set_title('Model accuracy when evaluated against noisy data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define new model to be trained on noisy data. The final layer density is higher than previously to account for the increased difficulty the network will encounter during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(128,activation='relu'),\n",
    "    keras.layers.Dense(20)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.SGD(learning_rate=1.0),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model on noisy data. With increasing noise, the accuracy drops substantially. At 0.1, the accuracy with 0 noise is mostly maintained, however by 0.2 the accuracy drops to 0.1 be the end of the training. At earlier epochs however, the accuracy is ~0.7, but this drops rapidly with further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with noisy data\n",
    "train_noises = np.linspace(0, 3, 7)\n",
    "acc_1 = trainWithNoise(model, image_x, image_y, 0.1, 100, 20)\n",
    "print(acc_1)\n",
    "\n",
    "# test model accuracy against noisy data\n",
    "noises = np.linspace(0, 2, 20)\n",
    "accuracies = np.empty([len(noises)])\n",
    "\n",
    "for i in range(len(noises)):\n",
    "    print(f\"y_noise = {noises[i]}:\")\n",
    "    test_loss, test_acc = accuracyWithNoise(model, image_x, image_y, noises[i])\n",
    "    accuracies[i] = test_acc\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(noises, accuracies)\n",
    "ax.set_xlabel('Maximum noise level')\n",
    "ax.set_ylabel('Model accuracy')\n",
    "ax.set_title('Model accuracy when evaluated against noisy data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When tested against the noisy data, the model trained on noisy data performs much better than the model trained on clean data. Although when tested on clean data they perform similarly, at greater noise levels in the test data, the accuracy of the model trained on noisy data drops much slower than that of the model trained on clean data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
