{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Machine Learning for Physicists\n",
    "## Week 4 Exercise - Part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both of these tasks we want you to implement autoencoder networks that:\n",
    "- Train on randomly generated circles (using the circle_generator function below)\n",
    "- Use 27x27 pixel images\n",
    "- Use no more than 30,000 randomly generated samples (e.g. batchsize 30 and 1000 steps, or batchsize 1000 and 30 steps, or anywhere inbetween) in training the final networks for each task\n",
    "- Use the mean_squared_error loss function\n",
    "- Fulfil the network size requirement listed in the task (can be verifired using the print_layers function, after the network is partially trained)\n",
    "\n",
    "### Task 1:\n",
    "Implement any network design, but the bottleneck must contain no more than 9 neurons.\n",
    "\n",
    "### Task 2:\n",
    "Implement any network design, but the bottleneck must contain no more than 3 neurons.\n",
    "\n",
    "\n",
    "\n",
    "#### Practicalities\n",
    "You should use this notebook for your work and upload it to both Moodle and CoCalc. You are expected to use TensorFlow and Keras to complete these tasks. The notebook should be self-contained and able to be executed if necessary. Marks will be awarded for (roughly equally weighted):\n",
    "- Overall notebook clarity (both in terms of good coding practice and coherent discussion)\n",
    "- Task 1 performance (0.02 is a good target cost to do better than)\n",
    "- Task 2 performance ( a good target here is left for the student to determine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 11:00:57.005100: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-08 11:00:57.412911: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-08 11:00:57.680420: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-08 11:00:57.680475: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-08 11:00:59.079102: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-08 11:00:59.079207: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-08 11:00:59.079216: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A big messy function to do the training\n",
    "# model -- our keras neural model autoencoder\n",
    "# image_generator -- a function to generate random images for the training (see below for examples)\n",
    "# img_size -- the size of our image in pixels\n",
    "# batchsize -- the number of images to include in each training batch\n",
    "# steps -- the number of steps taken in the training\n",
    "#\n",
    "# returns an array of the costs\n",
    "def generate_and_train(model,image_generator,img_size,batchsize,steps):\n",
    "   \n",
    "    #Generate an array of the numbers 1 to img_size and create a meshgrid from them    \n",
    "    pixels=np.linspace(-1,1,img_size)\n",
    "    x,y=np.meshgrid(pixels,pixels)\n",
    "    \n",
    "    #Now create a test image using 1 call to image_generator\n",
    "    #y_test=np.zeros([1,pixels,pixels,1])\n",
    "    #y_test[:,:,:,0]=image_generator(1,x,y)\n",
    "    \n",
    "    #Now create the empty arrays for the images and cost\n",
    "    y_in=np.zeros([batchsize,img_size,img_size,1])\n",
    "    y_target=np.zeros([batchsize,img_size,img_size,1])\n",
    "    cost=np.zeros(steps)\n",
    "    \n",
    "    #Loop through the steps, get a random batch of samples, train the model, repeat\n",
    "    for k in range(steps):\n",
    "        # produce samples:\n",
    "        y_in[:,:,:,0]=image_generator(batchsize,x,y)\n",
    "        y_target=np.copy(y_in) # autoencoder wants to reproduce its input!\n",
    "        \n",
    "        # do one training step on this batch of samples:\n",
    "        cost[k]=model.train_on_batch(y_in,y_target)\n",
    "    \n",
    "    return cost,y_target\n",
    "\n",
    "def get_test_image(image_generator,img_size):\n",
    "    #Generate an array of the numbers 1 to img_size and create a meshgrid from them    \n",
    "    pixels=np.linspace(-1,1,img_size)\n",
    "    x,y=np.meshgrid(pixels,pixels)\n",
    "    \n",
    "    #Now create a test image using 1 call to image_generator\n",
    "    y_test=np.zeros([1,img_size,img_size,1])\n",
    "    y_test[:,:,:,0]=image_generator(1,x,y)\n",
    "    return y_test\n",
    "\n",
    "# A function to generate and plot a single test image and the output of our model\n",
    "# only to be called after training the model\n",
    "def plot_test_image(model,image_generator,img_size):\n",
    "    #Get random test image\n",
    "    y_test=get_test_image(image_generator,img_size)\n",
    "    \n",
    "    #Create the output image\n",
    "    y_test_out=model.predict_on_batch(y_test)\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    ax[0].imshow(y_test[0,:,:,0],origin='lower')\n",
    "    ax[0].set_title(\"Input\")\n",
    "    ax[1].imshow(y_test_out[0,:,:,0],origin='lower')\n",
    "    ax[1].set_title(\"Output\")\n",
    "    \n",
    "def print_layers(network, y_in):\n",
    "    \"\"\"\n",
    "    Call this on some test images y_in, to get a print-out of\n",
    "    the layer sizes. Shapes shown are (batchsize,pixels,pixels,channels).\n",
    "    After a call to the visualization routine, y_target will contain\n",
    "    the last set of training images, so you could feed those in here.\n",
    "    \"\"\"\n",
    "    layer_features=get_layer_activations(network,y_in)\n",
    "    #print(layer_features)\n",
    "    for idx,feature in enumerate(layer_features):\n",
    "        s=np.shape(feature)\n",
    "        print(\"Layer \"+str(idx)+\": \"+str(s[1]*s[2]*s[3])+\" neurons / \", s)\n",
    "\n",
    "def get_layer_activation_extractor(network):\n",
    "    #print(network.inputs)\n",
    "    #for layer in network.layers:\n",
    "    #    print(layer.output)\n",
    "    return(keras.Model(inputs=network.inputs,\n",
    "                            outputs=[layer.output for layer in network.layers]))\n",
    "\n",
    "def get_layer_activations(network, y_in):\n",
    "    \"\"\"\n",
    "    Call this on some test images y_in, to get the intermediate \n",
    "    layer neuron values. These are returned in a list, with one\n",
    "    entry for each layer (the entries are arrays).\n",
    "    \"\"\"\n",
    "    extractor=get_layer_activation_extractor(network)\n",
    "    #print(extractor)\n",
    "    layer_features = extractor(y_in)\n",
    "    return layer_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circle generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple image generator that returns an array of batchsize images\n",
    "# each image has a size of x * y pixels\n",
    "# in this image each image has a randomly placed circle (and the circle is of random size)\n",
    "def generate_circle(batchsize,x,y):\n",
    "    R=np.random.uniform(size=batchsize)\n",
    "    x0=np.random.uniform(size=batchsize,low=-1,high=1)\n",
    "    y0=np.random.uniform(size=batchsize,low=-1,high=1)\n",
    "    return( 1.0*((x[None,:,:]-x0[:,None,None])**2 + (y[None,:,:]-y0[:,None,None])**2 < R[:,None,None]**2) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_20 (Conv2D)          (None, None, None, 4)     104       \n",
      "                                                                 \n",
      " average_pooling2d_4 (Averag  (None, None, None, 4)    0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, None, None, 4)     404       \n",
      "                                                                 \n",
      " average_pooling2d_5 (Averag  (None, None, None, 4)    0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, None, None, 1)     37        \n",
      "                                                                 \n",
      " up_sampling2d_4 (UpSampling  (None, None, None, 1)    0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, None, None, 4)     104       \n",
      "                                                                 \n",
      " up_sampling2d_5 (UpSampling  (None, None, None, 4)    0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, None, None, 4)     404       \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, None, None, 1)     37        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,090\n",
      "Trainable params: 1,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2=keras.models.Sequential()\n",
    "# 3x3 kernel size, 10 channels in first hidden layer:\n",
    "model2.add(keras.layers.Conv2D(4, 5, input_shape = (None, None, 1), activation = \"sigmoid\", padding = 'same'))\n",
    "model2.add(keras.layers.AveragePooling2D(pool_size = (3,3), padding = 'same')) # down\n",
    "model2.add(keras.layers.Conv2D(4, 5, activation = \"sigmoid\", padding = 'same'))\n",
    "model2.add(keras.layers.AveragePooling2D(pool_size = (3,3), padding = 'same')) # down\n",
    "model2.add(keras.layers.Conv2D(1, 3, activation = \"sigmoid\", padding = 'same'))\n",
    "model2.add(keras.layers.UpSampling2D(size = (3, 3))) # up\n",
    "model2.add(keras.layers.Conv2D(4, 5, activation = \"sigmoid\", padding = 'same'))\n",
    "model2.add(keras.layers.UpSampling2D(size = (3,3))) # up\n",
    "model2.add(keras.layers.Conv2D(4, 5, activation = \"sigmoid\", padding = 'same'))\n",
    "model2.add(keras.layers.Conv2D(1, 3, activation = \"linear\", padding = 'same'))\n",
    "\n",
    "model2.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/losses.py\", line 1500, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 9 and 27 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential_6/conv2d_25/BiasAdd, IteratorGetNext:1)' with input shapes: [100,9,27,1], [100,27,27,1].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/rhtwe/UCL/machine_learning_for_physicists/ml_assignments/Week4_AutoencoderChallenge.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rhtwe/UCL/machine_learning_for_physicists/ml_assignments/Week4_AutoencoderChallenge.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Now we can do the actual model training\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rhtwe/UCL/machine_learning_for_physicists/ml_assignments/Week4_AutoencoderChallenge.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m steps \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rhtwe/UCL/machine_learning_for_physicists/ml_assignments/Week4_AutoencoderChallenge.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m cost, y_target \u001b[39m=\u001b[39m generate_and_train(model2, generate_circle, img_size \u001b[39m=\u001b[39;49m \u001b[39m9\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m3\u001b[39;49m, batchsize \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m, steps \u001b[39m=\u001b[39;49m steps)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rhtwe/UCL/machine_learning_for_physicists/ml_assignments/Week4_AutoencoderChallenge.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#Plot the cost\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rhtwe/UCL/machine_learning_for_physicists/ml_assignments/Week4_AutoencoderChallenge.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots()\n",
      "\u001b[1;32m/home/rhtwe/UCL/machine_learning_for_physicists/ml_assignments/Week4_AutoencoderChallenge.ipynb Cell 8\u001b[0m in \u001b[0;36mgenerate_and_train\u001b[0;34m(model, image_generator, img_size, batchsize, steps)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rhtwe/UCL/machine_learning_for_physicists/ml_assignments/Week4_AutoencoderChallenge.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     y_target\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mcopy(y_in) \u001b[39m# autoencoder wants to reproduce its input!\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rhtwe/UCL/machine_learning_for_physicists/ml_assignments/Week4_AutoencoderChallenge.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m# do one training step on this batch of samples:\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/rhtwe/UCL/machine_learning_for_physicists/ml_assignments/Week4_AutoencoderChallenge.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     cost[k]\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39;49mtrain_on_batch(y_in,y_target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rhtwe/UCL/machine_learning_for_physicists/ml_assignments/Week4_AutoencoderChallenge.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m cost,y_target\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py:2478\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2474\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39msingle_batch_iterator(\n\u001b[1;32m   2475\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[1;32m   2476\u001b[0m     )\n\u001b[1;32m   2477\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[0;32m-> 2478\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   2480\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2481\u001b[0m \u001b[39mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file1d6tok2d.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py:1233\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1229\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[1;32m   1230\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m     )\n\u001b[1;32m   1232\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1233\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[1;32m   1234\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1235\u001b[0m     outputs,\n\u001b[1;32m   1236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[1;32m   1237\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[1;32m   1238\u001b[0m )\n\u001b[1;32m   1239\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py:1222\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1222\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m   1223\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py:1024\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m   1023\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m-> 1024\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[1;32m   1025\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[1;32m   1026\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py:1082\u001b[0m, in \u001b[0;36mModel.compute_loss\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[39m\"\"\"Compute the total loss, validate it, and return it.\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \n\u001b[1;32m   1033\u001b[0m \u001b[39mSubclasses can optionally override this method to provide custom loss\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[39m  is the case when called by `Model.test_step`).\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[39mdel\u001b[39;00m x  \u001b[39m# The default implementation does not use `x`.\u001b[39;00m\n\u001b[0;32m-> 1082\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompiled_loss(\n\u001b[1;32m   1083\u001b[0m     y, y_pred, sample_weight, regularization_losses\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlosses\n\u001b[1;32m   1084\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/compile_utils.py:265\u001b[0m, in \u001b[0;36mLossesContainer.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight, regularization_losses)\u001b[0m\n\u001b[1;32m    263\u001b[0m y_t, y_p, sw \u001b[39m=\u001b[39m match_dtype_and_rank(y_t, y_p, sw)\n\u001b[1;32m    264\u001b[0m sw \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mapply_mask(y_p, sw, losses_utils\u001b[39m.\u001b[39mget_mask(y_p))\n\u001b[0;32m--> 265\u001b[0m loss_value \u001b[39m=\u001b[39m loss_obj(y_t, y_p, sample_weight\u001b[39m=\u001b[39;49msw)\n\u001b[1;32m    267\u001b[0m total_loss_mean_value \u001b[39m=\u001b[39m loss_value\n\u001b[1;32m    268\u001b[0m \u001b[39m# Correct for the `Mean` loss metrics counting each replica as a\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m# batch.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/losses.py:152\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     call_fn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m    149\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall, tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    150\u001b[0m     )\n\u001b[0;32m--> 152\u001b[0m losses \u001b[39m=\u001b[39m call_fn(y_true, y_pred)\n\u001b[1;32m    154\u001b[0m in_mask \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mget_mask(y_pred)\n\u001b[1;32m    155\u001b[0m out_mask \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mget_mask(losses)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/losses.py:284\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    277\u001b[0m     y_pred, y_true \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39msqueeze_or_expand_dimensions(\n\u001b[1;32m    278\u001b[0m         y_pred, y_true\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    281\u001b[0m ag_fn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m    282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn, tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    283\u001b[0m )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m ag_fn(y_true, y_pred, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fn_kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/losses.py:1500\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m   1498\u001b[0m y_pred \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(y_pred)\n\u001b[1;32m   1499\u001b[0m y_true \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(y_true, y_pred\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m-> 1500\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39mmean(tf\u001b[39m.\u001b[39;49mmath\u001b[39m.\u001b[39;49msquared_difference(y_pred, y_true), axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/rhtwe/.local/lib/python3.9/site-packages/keras/losses.py\", line 1500, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 9 and 27 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential_6/conv2d_25/BiasAdd, IteratorGetNext:1)' with input shapes: [100,9,27,1], [100,27,27,1].\n"
     ]
    }
   ],
   "source": [
    "#Now we can do the actual model training\n",
    "steps = 500\n",
    "cost, y_target = generate_and_train(model2, generate_circle, img_size = 9*3, batchsize = 100, steps = steps)\n",
    "#Plot the cost\n",
    "fig, ax = plt.subplots()\n",
    "stepArray=np.arange(steps) \n",
    "ax.plot(stepArray,cost,linewidth=3) \n",
    "ax.set_xlabel(\"Step Number\") \n",
    "ax.set_ylabel(\"Cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: 2916 neurons /  (100, 27, 27, 4)\n",
      "Layer 1: 324 neurons /  (100, 9, 9, 4)\n",
      "Layer 2: 324 neurons /  (100, 9, 9, 4)\n",
      "Layer 3: 36 neurons /  (100, 3, 3, 4)\n",
      "Layer 4: 9 neurons /  (100, 3, 3, 1)\n",
      "Layer 5: 81 neurons /  (100, 9, 9, 1)\n",
      "Layer 6: 324 neurons /  (100, 9, 9, 4)\n",
      "Layer 7: 2916 neurons /  (100, 27, 27, 4)\n",
      "Layer 8: 2916 neurons /  (100, 27, 27, 4)\n",
      "Layer 9: 729 neurons /  (100, 27, 27, 1)\n"
     ]
    }
   ],
   "source": [
    "print_layers(model2, y_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
