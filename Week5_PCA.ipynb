{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 - PCA and MNIST\n",
    "\n",
    "### Heather Tweedie, 16/02/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers\n",
    "import keras.datasets.mnist\n",
    "\n",
    "from scipy import linalg\n",
    "\n",
    "import matplotlib.style #Some style nonsense\n",
    "import matplotlib as mpl #Some more style nonsense\n",
    "\n",
    "#Set default figure size\n",
    "#mpl.rcParams['figure.figsize'] = [12.0, 8.0] #Inches... of course it is inches\n",
    "mpl.rcParams[\"legend.frameon\"] = False\n",
    "mpl.rcParams['figure.dpi']=200 # dots per inch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# re-scale inputs\n",
    "train_images=train_images/255.0\n",
    "test_images=test_images/255.0\n",
    "\n",
    "# check shape of datasets\n",
    "print(\"Shape of training images:\",train_images.shape)\n",
    "print(\"Length of training set labels:\",len(train_labels))\n",
    "print(\"First label:\",train_labels[0])\n",
    "print(\"Shape of testing images:\",test_images.shape)\n",
    "print(\"Length of testing set labels:\",len(test_labels))\n",
    "\n",
    "image_x = len(train_images[0,:,0])\n",
    "image_y = len(train_images[0,0,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define, compile and train model on MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(128,activation='relu'),\n",
    "    keras.layers.Dense(15)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.SGD(learning_rate=1.0),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, batch_size=100, epochs=15, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to be used in exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_components(images):\n",
    "    \"\"\"\n",
    "    Decomposes a set of images into PCA components.\n",
    "    \n",
    "    Params:\n",
    "        images: the set of images to be decomposed\n",
    "    \"\"\"\n",
    "\n",
    "    num_images = len(images[:,0,0])\n",
    "    X = np.reshape(images, (num_images, 784))\n",
    "    mu = np.mean(X, axis=0)\n",
    "    x = X - mu\n",
    "    rho = np.cov(x, rowvar = False)\n",
    "\n",
    "    #Get the eigenvalues and vectors\n",
    "    vals, vecs = linalg.eigh(rho)\n",
    "    #vals is a 1-D array of the eigenvalues in ascending order, vecs is a columnwise array of the normalised\n",
    "    # eigenvectors such that vecs[:,i] is the eigenvector correspondong to vals[i]\n",
    "    vecs = np.flip(vecs)\n",
    "    vals = np.flip(vals)\n",
    "\n",
    "    return vecs, vals, x, mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decompose MNIST dataset into PCA components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_train, vals_train, x_train, mu_train = get_components(train_images)\n",
    "\n",
    "print(np.shape(vals_train))\n",
    "fig,ax=plt.subplots()\n",
    "index=np.arange((np.shape(vals_train)[0]))\n",
    "ax.plot(index, vals_train.real, \".\")\n",
    "ax.set_xlabel(\"PCA Index\")\n",
    "ax.set_ylabel(\"Variance\")\n",
    "ax.set_title('Change in variance with cumulative PCA components')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model against N testing datasets, and plot accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_test, vals_test, x_test, mu_test = get_components(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model):\n",
    "    \"\"\"\n",
    "    Creates N test datasets of images and tests a given model against these, then plots the accuracies.\n",
    "    N is the number of the best components to be used in the test dataset images.\n",
    "    \n",
    "    Params:\n",
    "        model: the model to be tested\n",
    "    \"\"\"\n",
    "\n",
    "    N = [2, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 150, 200, 250, 300, 350, 400, 450, 500, 600, 700, 784]\n",
    "\n",
    "    test_accuracies = np.empty(len(N))\n",
    "    for i in range(len(N)):\n",
    "        P = np.dot(x_test, vecs_test)\n",
    "        new_test = (np.dot(P[:,0:N[i]], vecs_test.T[0:N[i],:])) + mu_test\n",
    "        test_loss, test_acc = model.evaluate(new_test, test_labels, verbose = 2)\n",
    "        test_accuracies[i] = test_acc\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(N, test_accuracies)\n",
    "    ax.set_xlabel('Number of PCA components used (first N)')\n",
    "    ax.set_ylabel('Test accuracy')\n",
    "    ax.set_title('Number of PCA components vs test accuracy')\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain network on best 100 PCA components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataset\n",
    "P = np.dot(x_train, vecs_train)\n",
    "new_training_100 = (np.dot(P[:,0:100], vecs_train.T[0:100,:])) + mu_train\n",
    "new_training_100 = np.reshape(new_training_100, (60000,28,28))\n",
    "\n",
    "# define, compile and train new model on new training dataset\n",
    "model_100 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(128,activation='relu'),\n",
    "    keras.layers.Dense(15)\n",
    "])\n",
    "\n",
    "model_100.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.SGD(learning_rate=1.0),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model_100.fit(new_training_100, train_labels, batch_size=100, epochs=15, validation_data=(test_images, test_labels))\n",
    "\n",
    "test_accuracy(model_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain network on best 20 PCA components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataset\n",
    "P = np.dot(x_train, vecs_train)\n",
    "new_training_20 = (np.dot(P[:,0:20], vecs_train.T[0:20,:])) + mu_train\n",
    "new_training_20 = np.reshape(new_training_20, (60000,28,28))\n",
    "\n",
    "# define, compile and train new model\n",
    "model_20 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(128,activation='relu'),\n",
    "    keras.layers.Dense(15)\n",
    "])\n",
    "\n",
    "model_20.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.SGD(learning_rate=1.0),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model_20.fit(new_training_20, train_labels, batch_size=100, epochs=15, validation_data=(test_images, test_labels))\n",
    "\n",
    "test_accuracy(model_20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
